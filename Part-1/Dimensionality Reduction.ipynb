{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "865308f2",
   "metadata": {},
   "source": [
    "When there are millions of features for training instance, it makes harder to find optimal solution and the training process will be very slow and this is referred as the curse of dimensionality.\n",
    "\n",
    "In real-world problems, we can reduce number of features considerably but reducing dimensionality does lose some information making the model a bit worse. It also makes the pipelines a bit more complex and thus harder to maintain. \n",
    "\n",
    "High dimensional datasets are at risk of being very sparse, which means a new instance will likely be far away from any training instances, making predictions much less reliable than in lower dimensions. \n",
    "\n",
    "The more dimensions the training set has, the greater the risk of overfitting it.\n",
    "\n",
    "\n",
    "#### Projection\n",
    "\n",
    "We can project the training instances into a much lower-dimensional subspace of the high-dimensional space.\n",
    "\n",
    "However, projection is not always the best approach to dimensionality reduction. In many cases the subspace may twist and turn, such as in the famous Swiss roll toy dataset. \n",
    "\n",
    "\n",
    "#### Manifold Learning\n",
    "It is based on the idea that high-dimensional data often lie on or near a lower-dimensional manifold within the higher-dimensional space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
