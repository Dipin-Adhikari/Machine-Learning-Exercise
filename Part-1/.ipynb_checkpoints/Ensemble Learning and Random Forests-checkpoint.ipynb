{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce543b93",
   "metadata": {},
   "source": [
    "#### Hard Voting Classifier\n",
    "\n",
    "We can create a better classifier by aggregating the predictions of each classifier and predict the class that gets the most votes. It is known as hard voting classifier.\n",
    "\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ffacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = SVC(gamma='auto', random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4739f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4692aac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(random_state=42,\n",
       "                                                 solver='liblinear')),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(n_estimators=10,\n",
       "                                                     random_state=42)),\n",
       "                             ('svc', SVC(gamma='auto', random_state=42))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41aceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.85\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.87\n",
      "VotingClassifier 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694471c8",
   "metadata": {},
   "source": [
    "#### Soft Voting Classifier\n",
    "\n",
    "It is method where the predictions of multiple individual models are combined to make a final prediction. It achieves higher performance than voting because it gives more weight to highly confident votes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c5734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = SVC(gamma='auto', probability=True,  random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5be5c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.85\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.87\n",
      "VotingClassifier 0.9\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822c020",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n",
    "\n",
    "Bagging uses the same training algorithm for every predictor, but to train them on different random subsets of the training set by sampling with replacement(Bootstraping). But if sampling is performed without replacement, it is called pasting.\n",
    "\n",
    "Scikit-learn offers BaggingClassifier class or BaggingRegressor class for both bagging and pasting. For pasting we can state the bootstrap hyperparameter to False. \n",
    "\n",
    "The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities.\n",
    "\n",
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting, but predictors end up being less correlated so the ensemble's variance is reduced. Overall, bagging often results in better models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d921b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(), n_estimators=500,\n",
    "            max_samples=100, bootstrap=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5b27c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9cb7f",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. The probability of not selecting n samples in n draws is (1-1/n)^n. When the value is big, we can approximate this probability to 1/e, which is 0.3678. This means when the dataset is big enough, 37% of its samples are never selected and we could use it to test our model. This is called Out-of-Bag Scoring, or OOB Scoring.\n",
    "\n",
    "In Scikit-learn, we can set oob_score=True when creating a BaggingClassifier to request an automatic oob evaluation after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4c25bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  oob_score=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "                DecisionTreeClassifier(), n_estimators=500,\n",
    "                bootstrap=True, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d34b29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9025"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51a5ac6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfbf96a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04102564, 0.95897436],\n",
       "       [0.96111111, 0.03888889],\n",
       "       [0.99421965, 0.00578035],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99435028, 0.00564972],\n",
       "       [0.63402062, 0.36597938],\n",
       "       [0.53513514, 0.46486486],\n",
       "       [0.01058201, 0.98941799],\n",
       "       [0.58469945, 0.41530055],\n",
       "       [0.91256831, 0.08743169],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.47368421, 0.52631579],\n",
       "       [0.97826087, 0.02173913],\n",
       "       [0.9939759 , 0.0060241 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98170732, 0.01829268],\n",
       "       [0.02604167, 0.97395833],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.81578947, 0.18421053],\n",
       "       [1.        , 0.        ],\n",
       "       [0.70520231, 0.29479769],\n",
       "       [0.40588235, 0.59411765],\n",
       "       [0.37313433, 0.62686567],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08287293, 0.91712707],\n",
       "       [0.39344262, 0.60655738],\n",
       "       [0.0212766 , 0.9787234 ],\n",
       "       [0.98907104, 0.01092896],\n",
       "       [0.97487437, 0.02512563],\n",
       "       [0.8814433 , 0.1185567 ],\n",
       "       [0.01058201, 0.98941799],\n",
       "       [0.84415584, 0.15584416],\n",
       "       [0.86144578, 0.13855422],\n",
       "       [0.95505618, 0.04494382],\n",
       "       [0.04347826, 0.95652174],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97687861, 0.02312139],\n",
       "       [0.94680851, 0.05319149],\n",
       "       [0.99456522, 0.00543478],\n",
       "       [0.01587302, 0.98412698],\n",
       "       [0.33157895, 0.66842105],\n",
       "       [0.90052356, 0.09947644],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97282609, 0.02717391],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.76243094, 0.23756906],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14444444, 0.85555556],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01086957, 0.98913043],\n",
       "       [0.4198895 , 0.5801105 ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.30508475, 0.69491525],\n",
       "       [0.36318408, 0.63681592],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01123596, 0.98876404],\n",
       "       [1.        , 0.        ],\n",
       "       [0.91959799, 0.08040201],\n",
       "       [0.94416244, 0.05583756],\n",
       "       [0.97340426, 0.02659574],\n",
       "       [0.        , 1.        ],\n",
       "       [0.04761905, 0.95238095],\n",
       "       [0.99441341, 0.00558659],\n",
       "       [0.01075269, 0.98924731],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01630435, 0.98369565],\n",
       "       [1.        , 0.        ],\n",
       "       [0.76923077, 0.23076923],\n",
       "       [0.3989071 , 0.6010929 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01098901, 0.98901099],\n",
       "       [0.71356784, 0.28643216],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.93513514, 0.06486486],\n",
       "       [1.        , 0.        ],\n",
       "       [0.58241758, 0.41758242],\n",
       "       [0.12953368, 0.87046632],\n",
       "       [0.71518987, 0.28481013],\n",
       "       [0.93513514, 0.06486486],\n",
       "       [0.        , 1.        ],\n",
       "       [0.21354167, 0.78645833],\n",
       "       [0.95180723, 0.04819277],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98918919, 0.01081081],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06417112, 0.93582888],\n",
       "       [0.0326087 , 0.9673913 ],\n",
       "       [0.36170213, 0.63829787],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00555556, 0.99444444],\n",
       "       [0.89071038, 0.10928962],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.24581006, 0.75418994],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00534759, 0.99465241],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [0.89772727, 0.10227273],\n",
       "       [0.75977654, 0.24022346],\n",
       "       [0.00520833, 0.99479167],\n",
       "       [1.        , 0.        ],\n",
       "       [0.24120603, 0.75879397],\n",
       "       [0.67213115, 0.32786885],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0326087 , 0.9673913 ],\n",
       "       [0.6954023 , 0.3045977 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01744186, 0.98255814],\n",
       "       [1.        , 0.        ],\n",
       "       [0.27710843, 0.72289157],\n",
       "       [0.5       , 0.5       ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98378378, 0.01621622],\n",
       "       [0.27918782, 0.72081218],\n",
       "       [0.95977011, 0.04022989],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.83333333, 0.16666667],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02222222, 0.97777778],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99521531, 0.00478469],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.93641618, 0.06358382],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00543478, 0.99456522],\n",
       "       [0.28089888, 0.71910112],\n",
       "       [0.98888889, 0.01111111],\n",
       "       [0.13812155, 0.86187845],\n",
       "       [0.99456522, 0.00543478],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.70967742, 0.29032258],\n",
       "       [0.39673913, 0.60326087],\n",
       "       [0.64912281, 0.35087719],\n",
       "       [0.81656805, 0.18343195],\n",
       "       [0.92307692, 0.07692308],\n",
       "       [0.03888889, 0.96111111],\n",
       "       [0.84705882, 0.15294118],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01657459, 0.98342541],\n",
       "       [0.98265896, 0.01734104],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02094241, 0.97905759],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01530612, 0.98469388],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95294118, 0.04705882],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.77720207, 0.22279793],\n",
       "       [0.42372881, 0.57627119],\n",
       "       [0.00574713, 0.99425287],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22916667, 0.77083333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99465241, 0.00534759],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01111111, 0.98888889],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98876404, 0.01123596],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.62176166, 0.37823834],\n",
       "       [0.91534392, 0.08465608],\n",
       "       [0.00520833, 0.99479167],\n",
       "       [0.98963731, 0.01036269],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.07894737, 0.92105263],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02617801, 0.97382199],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.02298851, 0.97701149],\n",
       "       [1.        , 0.        ],\n",
       "       [0.94285714, 0.05714286],\n",
       "       [0.77604167, 0.22395833],\n",
       "       [0.62962963, 0.37037037],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10169492, 0.89830508],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96531792, 0.03468208],\n",
       "       [0.98734177, 0.01265823],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01595745, 0.98404255],\n",
       "       [0.        , 1.        ],\n",
       "       [0.41142857, 0.58857143],\n",
       "       [0.89655172, 0.10344828],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99444444, 0.00555556],\n",
       "       [0.01734104, 0.98265896],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96276596, 0.03723404],\n",
       "       [0.        , 1.        ],\n",
       "       [0.22797927, 0.77202073],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00543478, 0.99456522],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97546012, 0.02453988],\n",
       "       [0.84530387, 0.15469613],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01265823, 0.98734177],\n",
       "       [0.08648649, 0.91351351],\n",
       "       [0.99470899, 0.00529101],\n",
       "       [0.02994012, 0.97005988],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05555556, 0.94444444],\n",
       "       [0.99507389, 0.00492611],\n",
       "       [0.86516854, 0.13483146],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97938144, 0.02061856],\n",
       "       [0.98324022, 0.01675978],\n",
       "       [0.07303371, 0.92696629],\n",
       "       [0.21590909, 0.78409091],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.40555556, 0.59444444],\n",
       "       [0.98947368, 0.01052632],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99444444, 0.00555556],\n",
       "       [0.        , 1.        ],\n",
       "       [0.46596859, 0.53403141],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01086957, 0.98913043],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01005025, 0.98994975],\n",
       "       [0.        , 1.        ],\n",
       "       [0.12371134, 0.87628866],\n",
       "       [0.15294118, 0.84705882],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.46666667, 0.53333333],\n",
       "       [0.0984456 , 0.9015544 ],\n",
       "       [0.64622642, 0.35377358],\n",
       "       [0.57458564, 0.42541436],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01058201, 0.98941799],\n",
       "       [0.        , 1.        ],\n",
       "       [0.50299401, 0.49700599],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.33157895, 0.66842105],\n",
       "       [0.85427136, 0.14572864],\n",
       "       [0.0923913 , 0.9076087 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.7989418 , 0.2010582 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.10880829, 0.89119171],\n",
       "       [0.04891304, 0.95108696],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.91351351, 0.08648649],\n",
       "       [0.26404494, 0.73595506],\n",
       "       [0.93532338, 0.06467662],\n",
       "       [0.01025641, 0.98974359],\n",
       "       [0.69480519, 0.30519481],\n",
       "       [0.05376344, 0.94623656],\n",
       "       [0.9947644 , 0.0052356 ],\n",
       "       [0.77595628, 0.22404372],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.90659341, 0.09340659],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.24852071, 0.75147929],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01030928, 0.98969072],\n",
       "       [0.87845304, 0.12154696],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.88770053, 0.11229947],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.75742574, 0.24257426],\n",
       "       [0.54639175, 0.45360825],\n",
       "       [0.        , 1.        ],\n",
       "       [0.91812865, 0.08187135],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.89340102, 0.10659898],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.72727273, 0.27272727],\n",
       "       [0.13333333, 0.86666667],\n",
       "       [0.36931818, 0.63068182],\n",
       "       [0.18518519, 0.81481481],\n",
       "       [0.        , 1.        ],\n",
       "       [0.90243902, 0.09756098],\n",
       "       [0.82122905, 0.17877095],\n",
       "       [0.00518135, 0.99481865],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00549451, 0.99450549],\n",
       "       [0.91860465, 0.08139535],\n",
       "       [0.94358974, 0.05641026],\n",
       "       [1.        , 0.        ],\n",
       "       [0.53926702, 0.46073298],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.01910828, 0.98089172],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.96407186, 0.03592814],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03763441, 0.96236559],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99431818, 0.00568182],\n",
       "       [0.01129944, 0.98870056],\n",
       "       [1.        , 0.        ],\n",
       "       [0.1746988 , 0.8253012 ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.36320755, 0.63679245],\n",
       "       [0.08252427, 0.91747573],\n",
       "       [0.1420765 , 0.8579235 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.97927461, 0.02072539],\n",
       "       [0.20670391, 0.79329609],\n",
       "       [0.99481865, 0.00518135],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99428571, 0.00571429],\n",
       "       [0.32446809, 0.67553191],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98907104, 0.01092896],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03157895, 0.96842105],\n",
       "       [0.98076923, 0.01923077],\n",
       "       [1.        , 0.        ],\n",
       "       [0.0310559 , 0.9689441 ],\n",
       "       [0.83139535, 0.16860465]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_ # it returns the class probabilites for each training instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebb525",
   "metadata": {},
   "source": [
    "#### Random Patches and Random Subspaces\n",
    "\n",
    "The BaggingClassifier class supports sampling the features as well. This can be controlled by two hyperparameters: max_features and bootstrap_features. \n",
    "This is useful when dealing with high-dimensional inputs such as images. \n",
    "\n",
    "Sampling both training instances and features is called the Random Patches method.\n",
    "\n",
    "Keeping all training instances (i.e. bootstrap=False and max_samples=1) but sampling features (i.e. bootstrap_features=True and/or max_features smaller than 1) is called the Random Subspaces method.\n",
    "\n",
    "Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d9af3",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90a3354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda686e",
   "metadata": {},
   "source": [
    "#### Extra-Trees\n",
    "\n",
    "We can make trees more random by using random thresholds for each feature rather than searching for the best possible thresholds and it is called an Extremely Randomized Trees ensemble or Extra-Trees. But this trades more bias for a lower variance. It also makes Extra Trees much faster to train than regular Random Forest since finding the best possible threshold for each feature at every node is time consuming.\n",
    "\n",
    "We can create an Extra-Trees Classifier using Scikit-learn's ExtraTreesClassifier class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25d9aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extra_tree_clf = ExtraTreesClassifier(n_estimators=500)\n",
    "extra_tree_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_extra_tree = extra_tree_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18e6b3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_extra_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e4da3",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "In Decision Tree, important features are likely to appear closer to the root of the tree, while unimportant features will often appear closer to the leaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3823d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c86e6a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.0823435903719625\n",
      "sepal width (cm) 0.021975901159818578\n",
      "petal length (cm) 0.4419019637204188\n",
      "petal width (cm) 0.45377854474780016\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41326269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c00ba214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rnd_clf.fit(mnist[\"data\"], mnist[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f84954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "def plot_digit(data):\n",
    "    img = data.reshape(28, 28)\n",
    "    plt.imshow(img, cmap=mpl.cm.hot, interpolation='nearest')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba4fbb6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(rnd_clf.feature_importances_)\n",
    "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f514cb95",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "Boosting (Hypothesis boosting) refers to combining several weak learners into a strong learner.\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "The core principle of it is to fit a sequence of weak learners on repeateadly modified versions of the data. The data modification at each boosting iteration consists of applying weights w1, w2,..., wn to each of the training samples. Initially, those weights are set to 1/N. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly.\n",
    "\n",
    "One Drawback of AdaBoost is it cannot be parallelized since each predictor can only be trained after the previous predictor has been trained and evaluated.\n",
    "\n",
    "AdaBoost can be used both for classification and regression problems. For multi-class classification, AdaBoostClassifier implements AdaBoost.SAMME (SAMME stands for Stagewise Additive Modeling using a Multiclass Exponential loss function), whereas for regression, AdaBoostRegressor implements AdaBoost.R2.\n",
    "\n",
    "But if the predictors can estimate class probabilities, Scikit-Learn can use a variant of SAMME called SAMME.R (R stands for Real) as it performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c924534a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "                DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "                algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceb224d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ada_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f44948",
   "metadata": {},
   "source": [
    "#### Gradient Boosting\n",
    "\n",
    "It tries to fit hte new predictor to the residual errors made by the previous predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32fc2f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3 * X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8feef495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8429b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = y - tree_reg1.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf312b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "283b1a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d207a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87e134ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8aae5d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75026781])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f2185ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1)\n",
    "gbrt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b18a528a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=84)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\n",
    "bst_n_estimators = np.argmin(errors)\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82f5859f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst_n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c0cf8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90428fa1",
   "metadata": {},
   "source": [
    "### Stacking\n",
    "\n",
    "Stacking also known as stacked generalization, is an ensemble learning technique, where multiple models (often referred as base learners or level 0 models) are trained to make predictions, and then a meta-model is trained to combine the predictions of these base models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f6c8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=10)),\n",
    "    ('gbdt', GradientBoostingClassifier())\n",
    "]\n",
    "\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef8b00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9fd6527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackingClassifier(cv=10,\n",
       "                   estimators=[('rf',\n",
       "                                RandomForestClassifier(n_estimators=10,\n",
       "                                                       random_state=42)),\n",
       "                               ('knn', KNeighborsClassifier(n_neighbors=10)),\n",
       "                               ('gbdt', GradientBoostingClassifier())],\n",
       "                   final_estimator=LogisticRegression())"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b8791b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a68bb60",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?\n",
    "--> Yes, we can get better results by just using voting ensemble if the models are very different to each other. Also we can use the concept of bagging and pasting so that the training instances could also be different.\n",
    "___\n",
    "\n",
    "\n",
    " 2. What is the difference between hard and soft voting classifiers?\n",
    "--> Hard voting classifiers focuses on most votes of the prediction by each model, whereas soft voting classifiers focuses on the probabilites by each model. \n",
    "___\n",
    "\n",
    "\n",
    "3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?\n",
    "--> Yes it is possible to speed up training of a bagging ensemble by distributing it across multiple servers. We can do same for pasting ensembles and random forests as each predictor in the ensemble is independent of the others. But for boosting ensembles, each predictor is built based on previous predictor, so training is sequential. For stacking each predictors in a given layer are independent, so they can be trained on parrallel across multiple servers, but for second or say n layer, the training of n-1 layer should have been completed.\n",
    "___\n",
    "\n",
    "\n",
    " 4. What is the benefit of out-of-bag evaluation?\n",
    "--> With OOB evaluation, each predictor can be evaluated using instances not trained making the predictor more accurate as there will be more traininig instances because we don't need to separate validation or test sets.\n",
    "___\n",
    "\n",
    "\n",
    " 5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Random Forests?\n",
    " --> Extra-Trees uses random threshold for each feature making them more random than regular Random Forests. This extra randomness acts like a form of regularization. As there is no process for searching the best possible thresholds, it is faster to train.\n",
    " ___\n",
    " \n",
    " \n",
    " 6. If your AdaBoost ensemble underfits the training data, what hyperparameters should you tweak and how?\n",
    " --> We can try increasing the number of estimators or reducing the regularization hyperparameters of the base estimator. We can also try increasing the learning rate.\n",
    " ___\n",
    " \n",
    " \n",
    " 7. If your Gradient Boosting ensemble overfits the training set, should you increase or decrease the learning rate?\n",
    " --> We should decrease it and even implement early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087776e6",
   "metadata": {},
   "source": [
    "#### 8. \n",
    "Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g., use the first 40,000 instances for training, the next 10,000 for validation, and the last 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fc99761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfa1acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1305f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2370eabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "659047d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances are 49000.\n",
      "Number of testing instances are 10500.\n",
      "Number of validation instances are 10500.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training instances are {len(X_train)}.\")\n",
    "print(f\"Number of testing instances are {len(X_test)}.\")\n",
    "print(f\"Number of validation instances are {len(X_val)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d88fa67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ML\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "D:\\ML\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(n_estimators=10, random_state=42),\n",
       " ExtraTreesClassifier(n_estimators=10),\n",
       " LinearSVC(random_state=42),\n",
       " LogisticRegression()]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "extra_tree_clf = ExtraTreesClassifier(n_estimators=10)\n",
    "svm_clf = LinearSVC(random_state=42)\n",
    "log_clf = LogisticRegression()\n",
    "\n",
    "[clf.fit(X_train, y_train) for clf in [rnd_clf, extra_tree_clf, svm_clf, log_clf]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ee19374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9441904761904761,\n",
       " 0.9471428571428572,\n",
       " 0.8609523809523809,\n",
       " 0.9176190476190477]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf.score(X_val, y_val) for clf in [rnd_clf, extra_tree_clf, svm_clf, log_clf]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb8e058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "estimators = [\n",
    "    (\"rnd_Clf\", rnd_clf),\n",
    "    (\"extra_tree_clf\", extra_tree_clf),\n",
    "    (\"svm_clf\", svm_clf),\n",
    "    (\"log_clf\", log_clf)\n",
    "]\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2f14ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ML\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "D:\\ML\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rnd_Clf',\n",
       "                              RandomForestClassifier(n_estimators=10,\n",
       "                                                     random_state=42)),\n",
       "                             ('extra_tree_clf',\n",
       "                              ExtraTreesClassifier(n_estimators=10)),\n",
       "                             ('svm_clf', LinearSVC(random_state=42)),\n",
       "                             ('log_clf', LogisticRegression())])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "827091ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9461904761904761"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "010e3210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rnd_Clf',\n",
       "                              RandomForestClassifier(n_estimators=10,\n",
       "                                                     random_state=42)),\n",
       "                             ('extra_tree_clf',\n",
       "                              ExtraTreesClassifier(n_estimators=10)),\n",
       "                             ('svm_clf', None),\n",
       "                             ('log_clf', LogisticRegression())])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.set_params(svm_clf=None) # removing the svm classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8ca0ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(n_estimators=10, random_state=42),\n",
       " ExtraTreesClassifier(n_estimators=10),\n",
       " LinearSVC(random_state=42),\n",
       " LogisticRegression()]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7415d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "del voting_clf.estimators_[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f02cb8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(n_estimators=10, random_state=42),\n",
       " ExtraTreesClassifier(n_estimators=10),\n",
       " LogisticRegression()]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4f741e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9503809523809523"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4bd21d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf.voting = 'soft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4316b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9518095238095238"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04b27622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9441904761904761,\n",
       " 0.9471428571428572,\n",
       " 0.8609523809523809,\n",
       " 0.9176190476190477]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf.score(X_val, y_val) for clf in [rnd_clf, extra_tree_clf, svm_clf, log_clf]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "75c4d5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9535238095238096"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "254ff2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9437142857142857,\n",
       " 0.9500952380952381,\n",
       " 0.8604761904761905,\n",
       " 0.9217142857142857]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[clf.score(X_test, y_test) for clf in [rnd_clf, extra_tree_clf, svm_clf, log_clf]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c2283",
   "metadata": {},
   "source": [
    "###  9. \n",
    "Run the individual classifiers from the previous exercise to make predictions on\n",
    " the validation set, and create a new training set with the resulting predictions:\n",
    " each training instance is a vector containing the set of predictions from all your\n",
    " classifiers for an image, and the target is the images class. Congratulations, you\n",
    " have just trained a blender, and together with the classifiers they form a stacking\n",
    " ensemble! Now lets evaluate the ensemble on the test set. For each image in the\n",
    " test set, make predictions with all your classifiers, then feed the predictions to the\n",
    " blender to get the ensembles predictions. How does it compare to the voting clas\n",
    "sifier you trained earlier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "08fe34b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_predictions = np.empty((len(X_val), 4), dtype=np.float32)\n",
    "\n",
    "for index, clf in enumerate([rnd_clf, extra_tree_clf, svm_clf, log_clf]):\n",
    "    X_val_predictions[:, index] = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a64bee16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 4., 9., 6.],\n",
       "       [4., 4., 4., 4.],\n",
       "       [7., 7., 9., 7.],\n",
       "       ...,\n",
       "       [4., 4., 4., 4.],\n",
       "       [3., 3., 3., 3.],\n",
       "       [2., 2., 2., 2.]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0c9cf394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)\n",
    "rnd_forest_blender.fit(X_val_predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fd9a8959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.948"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_forest_blender.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d3eddb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = np.empty((len(X_test), 4), dtype=np.float32)\n",
    "\n",
    "for index, clf in enumerate([rnd_clf, extra_tree_clf, svm_clf, log_clf]):\n",
    "    X_test_predictions[:, index] = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30a62fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_forest_blender.predict(X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "003bdedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9519047619047619"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c1d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
