{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce543b93",
   "metadata": {},
   "source": [
    "#### Hard Voting Classifier\n",
    "\n",
    "We can create a better classifier by aggregating the predictions of each classifier and predict the class that gets the most votes. It is known as hard voting classifier.\n",
    "\n",
    "\n",
    "Ensemble methods work best when the predictors are as independent from one another as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ffacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = SVC(gamma='auto', random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4739f0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=500, noise=0.3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4692aac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(random_state=42,\n",
       "                                                 solver='liblinear')),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(n_estimators=10,\n",
       "                                                     random_state=42)),\n",
       "                             ('svc', SVC(gamma='auto', random_state=42))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41aceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.85\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.87\n",
      "VotingClassifier 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694471c8",
   "metadata": {},
   "source": [
    "#### Soft Voting Classifier\n",
    "\n",
    "It is method where the predictions of multiple individual models are combined to make a final prediction. It achieves higher performance than voting because it gives more weight to highly confident votes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c5734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver='liblinear', random_state=42)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "svm_clf = SVC(gamma='auto', probability=True,  random_state=42)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5be5c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.85\n",
      "RandomForestClassifier 0.88\n",
      "SVC 0.87\n",
      "VotingClassifier 0.9\n"
     ]
    }
   ],
   "source": [
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822c020",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n",
    "\n",
    "Bagging uses the same training algorithm for every predictor, but to train them on different random subsets of the training set by sampling with replacement(Bootstraping). But if sampling is performed without replacement, it is called pasting.\n",
    "\n",
    "Scikit-learn offers BaggingClassifier class or BaggingRegressor class for both bagging and pasting. For pasting we can state the bootstrap hyperparameter to False. \n",
    "\n",
    "The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities.\n",
    "\n",
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting, but predictors end up being less correlated so the ensemble's variance is reduced. Overall, bagging often results in better models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d921b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "            DecisionTreeClassifier(), n_estimators=500,\n",
    "            max_samples=100, bootstrap=True)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5b27c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b9cb7f",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. The probability of not selecting n samples in n draws is (1-1/n)^n. When the value is big, we can approximate this probability to 1/e, which is 0.3678. This means when the dataset is big enough, 37% of its samples are never selected and we could use it to test our model. This is called Out-of-Bag Scoring, or OOB Scoring.\n",
    "\n",
    "In Scikit-learn, we can set oob_score=True when creating a BaggingClassifier to request an automatic oob evaluation after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4c25bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=500,\n",
       "                  oob_score=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "                DecisionTreeClassifier(), n_estimators=500,\n",
    "                bootstrap=True, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d34b29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.915"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51a5ac6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfbf96a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98941799, 0.01058201],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05527638, 0.94472362],\n",
       "       [0.92307692, 0.07692308],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99428571, 0.00571429],\n",
       "       [0.65555556, 0.34444444],\n",
       "       [0.54098361, 0.45901639],\n",
       "       [0.00534759, 0.99465241],\n",
       "       [0.5989011 , 0.4010989 ],\n",
       "       [0.86885246, 0.13114754],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00510204, 0.99489796],\n",
       "       [0.52791878, 0.47208122],\n",
       "       [0.97674419, 0.02325581],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99444444, 0.00555556],\n",
       "       [0.03208556, 0.96791444],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.78010471, 0.21989529],\n",
       "       [1.        , 0.        ],\n",
       "       [0.71282051, 0.28717949],\n",
       "       [0.39779006, 0.60220994],\n",
       "       [0.29508197, 0.70491803],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0982659 , 0.9017341 ],\n",
       "       [0.44162437, 0.55837563],\n",
       "       [0.01075269, 0.98924731],\n",
       "       [0.98369565, 0.01630435],\n",
       "       [0.96256684, 0.03743316],\n",
       "       [0.85      , 0.15      ],\n",
       "       [0.01621622, 0.98378378],\n",
       "       [0.77005348, 0.22994652],\n",
       "       [0.84656085, 0.15343915],\n",
       "       [0.95833333, 0.04166667],\n",
       "       [0.02020202, 0.97979798],\n",
       "       [0.        , 1.        ],\n",
       "       [0.985     , 0.015     ],\n",
       "       [0.96721311, 0.03278689],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02808989, 0.97191011],\n",
       "       [0.31979695, 0.68020305],\n",
       "       [0.89010989, 0.10989011],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96551724, 0.03448276],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.74033149, 0.25966851],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.11235955, 0.88764045],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.40677966, 0.59322034],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.31182796, 0.68817204],\n",
       "       [0.38068182, 0.61931818],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00558659, 0.99441341],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01546392, 0.98453608],\n",
       "       [0.98843931, 0.01156069],\n",
       "       [0.91414141, 0.08585859],\n",
       "       [0.92670157, 0.07329843],\n",
       "       [0.95721925, 0.04278075],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03571429, 0.96428571],\n",
       "       [0.99450549, 0.00549451],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00595238, 0.99404762],\n",
       "       [0.99473684, 0.00526316],\n",
       "       [0.71428571, 0.28571429],\n",
       "       [0.38251366, 0.61748634],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01104972, 0.98895028],\n",
       "       [0.70491803, 0.29508197],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.91160221, 0.08839779],\n",
       "       [1.        , 0.        ],\n",
       "       [0.62564103, 0.37435897],\n",
       "       [0.13294798, 0.86705202],\n",
       "       [0.64893617, 0.35106383],\n",
       "       [0.93258427, 0.06741573],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25714286, 0.74285714],\n",
       "       [0.95027624, 0.04972376],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98830409, 0.01169591],\n",
       "       [0.        , 1.        ],\n",
       "       [0.06417112, 0.93582888],\n",
       "       [0.03314917, 0.96685083],\n",
       "       [0.35057471, 0.64942529],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.87634409, 0.12365591],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.26600985, 0.73399015],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.00552486, 0.99447514],\n",
       "       [0.90957447, 0.09042553],\n",
       "       [0.81725888, 0.18274112],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.24456522, 0.75543478],\n",
       "       [0.65989848, 0.34010152],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0621118 , 0.9378882 ],\n",
       "       [0.64171123, 0.35828877],\n",
       "       [0.99425287, 0.00574713],\n",
       "       [0.03296703, 0.96703297],\n",
       "       [1.        , 0.        ],\n",
       "       [0.26842105, 0.73157895],\n",
       "       [0.52272727, 0.47727273],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01069519, 0.98930481],\n",
       "       [0.98941799, 0.01058201],\n",
       "       [0.32768362, 0.67231638],\n",
       "       [0.86979167, 0.13020833],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.82010582, 0.17989418],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01104972, 0.98895028],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00613497, 0.99386503],\n",
       "       [0.93548387, 0.06451613],\n",
       "       [0.99404762, 0.00595238],\n",
       "       [0.03827751, 0.96172249],\n",
       "       [0.25388601, 0.74611399],\n",
       "       [0.97222222, 0.02777778],\n",
       "       [0.0989011 , 0.9010989 ],\n",
       "       [0.99459459, 0.00540541],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.68786127, 0.31213873],\n",
       "       [0.37912088, 0.62087912],\n",
       "       [0.65142857, 0.34857143],\n",
       "       [0.9040404 , 0.0959596 ],\n",
       "       [0.98369565, 0.01630435],\n",
       "       [0.03846154, 0.96153846],\n",
       "       [0.84236453, 0.15763547],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.05172414, 0.94827586],\n",
       "       [0.99468085, 0.00531915],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00507614, 0.99492386],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01657459, 0.98342541],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.95098039, 0.04901961],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.76243094, 0.23756906],\n",
       "       [0.39444444, 0.60555556],\n",
       "       [0.00598802, 0.99401198],\n",
       "       [0.        , 1.        ],\n",
       "       [0.27894737, 0.72105263],\n",
       "       [1.        , 0.        ],\n",
       "       [0.99401198, 0.00598802],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01030928, 0.98969072],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97938144, 0.02061856],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.59893048, 0.40106952],\n",
       "       [0.92090395, 0.07909605],\n",
       "       [0.00529101, 0.99470899],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.07954545, 0.92045455],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01538462, 0.98461538],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03608247, 0.96391753],\n",
       "       [1.        , 0.        ],\n",
       "       [0.92178771, 0.07821229],\n",
       "       [0.77248677, 0.22751323],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.18811881, 0.81188119],\n",
       "       [1.        , 0.        ],\n",
       "       [0.9673913 , 0.0326087 ],\n",
       "       [0.97058824, 0.02941176],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01058201, 0.98941799],\n",
       "       [0.        , 1.        ],\n",
       "       [0.45549738, 0.54450262],\n",
       "       [0.85964912, 0.14035088],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99453552, 0.00546448],\n",
       "       [0.01156069, 0.98843931],\n",
       "       [0.        , 1.        ],\n",
       "       [0.95930233, 0.04069767],\n",
       "       [0.        , 1.        ],\n",
       "       [0.25142857, 0.74857143],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98351648, 0.01648352],\n",
       "       [0.86597938, 0.13402062],\n",
       "       [0.99479167, 0.00520833],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.09677419, 0.90322581],\n",
       "       [1.        , 0.        ],\n",
       "       [0.02747253, 0.97252747],\n",
       "       [0.        , 1.        ],\n",
       "       [0.09090909, 0.90909091],\n",
       "       [1.        , 0.        ],\n",
       "       [0.82513661, 0.17486339],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97687861, 0.02312139],\n",
       "       [0.98324022, 0.01675978],\n",
       "       [0.0960452 , 0.9039548 ],\n",
       "       [0.19791667, 0.80208333],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.40116279, 0.59883721],\n",
       "       [0.97191011, 0.02808989],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.5326087 , 0.4673913 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.01058201, 0.98941799],\n",
       "       [0.        , 1.        ],\n",
       "       [0.08121827, 0.91878173],\n",
       "       [0.0960452 , 0.9039548 ],\n",
       "       [0.98850575, 0.01149425],\n",
       "       [0.01818182, 0.98181818],\n",
       "       [1.        , 0.        ],\n",
       "       [0.5       , 0.5       ],\n",
       "       [0.09137056, 0.90862944],\n",
       "       [0.65853659, 0.34146341],\n",
       "       [0.58469945, 0.41530055],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.00540541, 0.99459459],\n",
       "       [0.        , 1.        ],\n",
       "       [0.51530612, 0.48469388],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.37704918, 0.62295082],\n",
       "       [0.9       , 0.1       ],\n",
       "       [0.05494505, 0.94505495],\n",
       "       [1.        , 0.        ],\n",
       "       [0.86666667, 0.13333333],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.14689266, 0.85310734],\n",
       "       [0.03846154, 0.96153846],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98342541, 0.01657459],\n",
       "       [0.96      , 0.04      ],\n",
       "       [0.20833333, 0.79166667],\n",
       "       [0.92553191, 0.07446809],\n",
       "       [0.00938967, 0.99061033],\n",
       "       [0.64864865, 0.35135135],\n",
       "       [0.03208556, 0.96791444],\n",
       "       [0.97382199, 0.02617801],\n",
       "       [0.76363636, 0.23636364],\n",
       "       [0.        , 1.        ],\n",
       "       [0.99438202, 0.00561798],\n",
       "       [0.92168675, 0.07831325],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.29015544, 0.70984456],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.01081081, 0.98918919],\n",
       "       [0.90810811, 0.09189189],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.87165775, 0.12834225],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.74611399, 0.25388601],\n",
       "       [0.43243243, 0.56756757],\n",
       "       [0.        , 1.        ],\n",
       "       [0.89473684, 0.10526316],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.77325581, 0.22674419],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.74698795, 0.25301205],\n",
       "       [0.13333333, 0.86666667],\n",
       "       [0.48768473, 0.51231527],\n",
       "       [0.18888889, 0.81111111],\n",
       "       [0.        , 1.        ],\n",
       "       [0.94318182, 0.05681818],\n",
       "       [0.8556701 , 0.1443299 ],\n",
       "       [0.00543478, 0.99456522],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.03636364, 0.96363636],\n",
       "       [0.93785311, 0.06214689],\n",
       "       [0.95906433, 0.04093567],\n",
       "       [1.        , 0.        ],\n",
       "       [0.4852071 , 0.5147929 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.98958333, 0.01041667],\n",
       "       [0.01639344, 0.98360656],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.97126437, 0.02873563],\n",
       "       [0.        , 1.        ],\n",
       "       [0.07100592, 0.92899408],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.18367347, 0.81632653],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.43258427, 0.56741573],\n",
       "       [0.13636364, 0.86363636],\n",
       "       [0.2513369 , 0.7486631 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.96464646, 0.03535354],\n",
       "       [0.18181818, 0.81818182],\n",
       "       [0.98360656, 0.01639344],\n",
       "       [0.        , 1.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.98907104, 0.01092896],\n",
       "       [0.34636872, 0.65363128],\n",
       "       [0.98245614, 0.01754386],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.        , 1.        ],\n",
       "       [0.0308642 , 0.9691358 ],\n",
       "       [0.9893617 , 0.0106383 ],\n",
       "       [1.        , 0.        ],\n",
       "       [0.06521739, 0.93478261],\n",
       "       [0.835     , 0.165     ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_clf.oob_decision_function_ # it returns the class probabilites for each training instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebb525",
   "metadata": {},
   "source": [
    "#### Random Patches and Random Subspaces\n",
    "\n",
    "The BaggingClassifier class supports sampling the features as well. This can be controlled by two hyperparameters: max_features and bootstrap_features. \n",
    "This is useful when dealing with high-dimensional inputs such as images. \n",
    "\n",
    "Sampling both training instances and features is called the Random Patches method.\n",
    "\n",
    "Keeping all training instances (i.e. bootstrap=False and max_samples=1) but sampling features (i.e. bootstrap_features=True and/or max_features smaller than 1) is called the Random Subspaces method.\n",
    "\n",
    "Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d9af3",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a3354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda686e",
   "metadata": {},
   "source": [
    "#### Extra-Trees\n",
    "\n",
    "We can make trees more random by using random thresholds for each feature rather than searching for the best possible thresholds and it is called an Extremely Randomized Trees ensemble or Extra-Trees. But this trades more bias for a lower variance. It also makes Extra Trees much faster to train than regular Random Forest since finding the best possible threshold for each feature at every node is time consuming.\n",
    "\n",
    "We can create an Extra-Trees Classifier using Scikit-learn's ExtraTreesClassifier class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25d9aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "extra_tree_clf = ExtraTreesClassifier(n_estimators=500)\n",
    "extra_tree_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_extra_tree = extra_tree_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18e6b3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_extra_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e4da3",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "In Decision Tree, important features are likely to appear closer to the root of the tree, while unimportant features will often appear closer to the leaves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3823d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c86e6a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09461124475251456\n",
      "sepal width (cm) 0.023138774929102046\n",
      "petal length (cm) 0.43433141095148736\n",
      "petal width (cm) 0.44791856936689595\n"
     ]
    }
   ],
   "source": [
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41326269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c00ba214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=42)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rnd_clf.fit(mnist[\"data\"], mnist[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f84954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_digit(data):\n",
    "    img = data.reshape(28, 28)\n",
    "    plt.imshow(img, cmap=mpl.cm.hot, interpolation='nearest')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba4fbb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_digit(rnd_clf.feature_importances_)\n",
    "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])\n",
    "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f45caa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
