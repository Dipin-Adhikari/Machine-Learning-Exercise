{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0b7b6f",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding Gradients Problems\n",
    "\n",
    "The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way. It uses the computed gradient to update each parameter. But, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the vanishing gradients problem.\n",
    "\n",
    "Also, the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is called exploding gradients problem.\n",
    "\n",
    "\n",
    "### Glorot and He initialization\n",
    "\n",
    "For the signal to flow properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we also need the gradients to have equal variance before and after flowing through a layer in the reverse direction. It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons. But, the connection weights of each layer must be intialized randomly, where fanavg = (fanin + fanout)/2. This intialization strategy is called Xavier initialization or Glorot initialization. \n",
    "\n",
    "\n",
    "He initialization aims to maintain a stable variance of activations throughout the layers of the network, preventing the gradients from becoming too small or too large during the backpropagation process.\n",
    "\n",
    "\n",
    "For tanh, logistic or softmax activation function glorot intialization is preferred.\n",
    "\n",
    "For ReLU and its variants, He initializaiton is used.\n",
    "\n",
    "For SeLU, LeCun is used.\n",
    "\n",
    "\n",
    "By default, Keras uses Glorot initialization with a uniform distribution. We can change this to He initialization by setting `kernel_initializer=\"he_uniform\"` or `kernel_initializer=\"he_normal\"` when creating a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126940bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
