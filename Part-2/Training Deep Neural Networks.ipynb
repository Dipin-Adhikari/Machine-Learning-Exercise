{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f0b7b6f",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding Gradients Problems\n",
    "\n",
    "The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way. It uses the computed gradient to update each parameter. But, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the vanishing gradients problem.\n",
    "\n",
    "Also, the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is called exploding gradients problem.\n",
    "\n",
    "\n",
    "### Glorot and He initialization\n",
    "\n",
    "For the signal to flow properly, we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we also need the gradients to have equal variance before and after flowing through a layer in the reverse direction. It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons. But, the connection weights of each layer must be intialized randomly, where fanavg = (fanin + fanout)/2. This intialization strategy is called Xavier initialization or Glorot initialization. \n",
    "\n",
    "\n",
    "He initialization aims to maintain a stable variance of activations throughout the layers of the network, preventing the gradients from becoming too small or too large during the backpropagation process.\n",
    "\n",
    "\n",
    "For tanh, logistic or softmax activation function glorot intialization is preferred.\n",
    "\n",
    "For ReLU and its variants, He initializaiton is used.\n",
    "\n",
    "For SeLU, LeCun is used.\n",
    "\n",
    "\n",
    "By default, Keras uses Glorot initialization with a uniform distribution. We can change this to He initialization by setting `kernel_initializer=\"he_uniform\"` or `kernel_initializer=\"he_normal\"` when creating a layer.\n",
    "\n",
    "\n",
    "\n",
    "### Nonsaturating Activation Functions\n",
    "\n",
    "The ReLU activation function is not perfect as it suffers from a problem known as the dying ReLUs meaning they stop outputting anything other than 0. In some cases, more than half of the network's neurons are dead, especially if we used a large learning rate. \n",
    "\n",
    "A neuron dies, when its weighted sum of its input gets negative, and as by ReLU activation function the output or gradient of the negative value is 0. So, it just keeps outputting 0s.\n",
    "\n",
    "To solve it, we can use a variant of the ReLU function such as Leaky ReLU. It is defined as LeakyReLU(z) = max(az, z). The hyperparameter a defines how much the function leaks: it is the slope of the function for z < 0, and is typically set to 0.01. This small slope ensures that Leaky ReLU never die; they can go into a long coma, but they have a chance to eventually wake up.\n",
    "\n",
    "RReLU (Randomized Leaky ReLU), where a is picked randomly in a given range during training, and it is fixed to an average value during testing. It also performed fairly well and seemed to act as a regularizer. \n",
    "\n",
    "Parametric Leaky ReLU, where a is authorized to be learned during training (modified by backpropagation). This was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.\n",
    "\n",
    "Exponential Linear Unit (ELU) outperformed all the ReLU variants in their experiments. It takes on negative values when z < 0, which allows the unit to have an average output closer to 0. This helps alleviate the vanishing gradients problem. It has non zero gradient for z < 0, which avoids the dead neurons problem. At z = 0, the function is differential, so it helps Gradient Descent to speed up, since it will not bounce as much left and right of z = 0. The main drawback of ELU is that it is slower to compute than the ReLU and its variants, but during training this is compensated by the faster convergence rate. However, at test time an ELU network will be slower than a ReLU network.\n",
    "\n",
    "SELU (Scaled ELU) activation function will make the network self-normalize: the output of each layer will tend to preserve mean 0 and standard deviation 1 during training, which solves the vanishing/exploding gradients problem. To use it: the input features must be standardized, every hidden layers weight must also be initialized using the LeCun normal initialization, the networks architecture must be sequential.\n",
    "\n",
    "\n",
    "In general SELU > ELU > Leaky ReLU > ReLU > tanh > logistic. If the network's architecture prevents it from self-normalizing, then ELU may perform better than SELU. \n",
    "\n",
    "To use the leaky ReLU activation function, we must create a LeakyReLU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126940bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "leaky_relu = keras.layers.LeakyReLU(alpha=0.2)\n",
    "layer = keras.layers.Dense(10, activation=leaky_relu, kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5df0d",
   "metadata": {},
   "source": [
    "For SELU activation, we can set `activation=\"selu\"` and `kernel_initializer=\"lecun_normal\"` when creating a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28dc5042",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a18dbf",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "It consists of adding zero-centering and normalizing each input, then scaling and shifting the result using two new parameter vectors per layer: one for scaling and other for shifting. This operation lets the model to learn the optimal scale and mean of each of the layer's inputs.\n",
    "\n",
    "\n",
    "For testing, it is preferred to estimate the final statistics during training using a moving average of the layer's input means and standard deviations. Four parameter vectors are learned in each batch-normalized layer: gamma(the output scale vector) and beta(the output offset vector) are learned through regular backpropagation, and meu(the final input mean vector), and sigma(the final input standard deviation vector) are estimated using an exponential moving average. \n",
    "\n",
    "Due to BN, the vanishing gradients problem is strongly reduced, to the point that we could use saturating activation functions such as the tanh and even the logistic activation function. The networks are also much less sensitive to the weight initialization. Higher learning rates can be used, speeding up the learning process. BN also acts like a regularizer, reducing the need for other regularization techniques.\n",
    "\n",
    "Each epoch takes much more time when using BN. However, this is usually conuterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach the same performance.\n",
    "\n",
    "\n",
    "#### Implementing Batch Normalization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac7820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    " keras.layers.Flatten(input_shape=[28, 28]),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Dense(10, activation=\"softmax\")\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62027f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3459fbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9741f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adding the BN layers before the activation functions, rather than after. \n",
    "    \n",
    "model = keras.models.Sequential([\n",
    " keras.layers.Flatten(input_shape=[28, 28]),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Activation(\"elu\"),\n",
    " keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    " keras.layers.Activation(\"elu\"),\n",
    " keras.layers.BatchNormalization(),\n",
    " keras.layers.Dense(10, activation=\"softmax\")\n",
    " ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89da8c5",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "Another popular tecnique to lessen the exploding gradients problem is to simply clip the gradients during backpropagation so that they never exceed some threshold. This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs. \n",
    "\n",
    "In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or clipnorm argument when creating an optimizer. \n",
    "\n",
    "clipvalue may change the orientation of the gradient vector so for ensuring that Gradient Clipping doesn't change the direction of the gradient vector, clipnorm should be used instead of clipvalue. \n",
    "\n",
    "\n",
    "### Reusing Pretrained Layers\n",
    "\n",
    "Transfer learning is a machine learning technique where a model trained for a specific task is reused for a different but related task. Transfer learning allows the new model to benefit from the knowledge acquired from the previous task. Transfer learning can reduce the cost and time of building and training the new model.\n",
    "\n",
    "The output layer of the original model should usually be replaced since it is most likely not useful at all for the new task.\n",
    "\n",
    "Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task.\n",
    "\n",
    "At first, we should try freezing all the reused layers, then train model to see how it performs. Then unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. It is also useful to reduce the learning rate when we unfreeze reused layers: this will avoid wrecking their fine-tuned weights.\n",
    "\n",
    "\n",
    "### EWMA\n",
    "\n",
    "EWMA stands for Exponentially Weighted Moving Average. It's a statistical method used to smooth time series data and identify trends by giving more weight to recent observations while exponentially decreasing the weights for older data. \n",
    "\n",
    "\n",
    "### Momentum Optimization\n",
    "\n",
    "Momentum Optimizer in Deep Learning is a technique that reduces the time to train a model. The path of learning in mini-batch gradient descent is zig-zag, and not straight. Thus, some time gets wasted in moving in a zig-zag direction. Momentum Optimizer in Deep Learning smooths out the zig-zag path and makes it much straighter, thus reducing the time taken to train the model.\n",
    "\n",
    "Momentum Optimization cares about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector m (multiplied by the learning rate n), and it updates the weights by simply adding this momentum vector.  To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter β, simply called the momentum, which must be set between 0 (high friction) and 1(no friction). A typical momentum value is 0.9.\n",
    "\n",
    "\n",
    "Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. This is one of the reason to have a bit of friction in the system: it gets rid of these oscillations and thus speeds up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf8bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c394d783",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "\n",
    "The idea of NAG is to measure the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum. The only difference from vanilla Momentum optimization is that the gradient is measured at θ + βm rather than at θ. So, small improvements add up and NAG ends up being significantly faster than regular Momentum optimization. \n",
    "\n",
    "The problem with NAG is it can stop at local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a08400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57f6f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
