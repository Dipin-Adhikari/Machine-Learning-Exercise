{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf08dc0",
   "metadata": {},
   "source": [
    "### The Perceptron\n",
    "\n",
    "It is based on a slightly different artificial neuron called a Threshold Logic Unit (TLU), or a Linear Threshold Unit (LTU). The input and output are numbers and each input connection is associated with a weight. The TLU computes a weighted sum of its input, then applies a step function to that sum and output the result.\n",
    "\n",
    "The most common step function is the Heaviside step function, sometimes the sign function is used instead.\n",
    "\n",
    "\n",
    "A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs. When all the neurons in a layer are connected to every neuron in the previous layer, it is called a fully connected layer or a dense layer. An extra bias feature is generally added and it is represented using a special type of neuron called a bias neuron. \n",
    "\n",
    "Computing the ouputs of a fully connected layer:\n",
    "h(X) = f(XW + b)\n",
    "\n",
    "X represents the matrix of input features, and has one row per instance, one column per feature.\n",
    "\n",
    "The weight matrix W contains all the connection weights except for the ones from the bias neuron. It has one row per input neuron and one column per artificial neuron in the layer.\n",
    "\n",
    "The bias vector b contains all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron.\n",
    "\n",
    "The function f is called the activation function: when the artificial neurons are TLUs, it is a step function.\n",
    "\n",
    "\n",
    "\n",
    "Perceptron Learning rule (weight update)\n",
    "\n",
    "w(i, j)(next step) = w + n(yj - yj')xi\n",
    "\n",
    "\n",
    "w = connection weight between the ith input neuron and the jth output neuron.\n",
    "\n",
    "xi = ith input value of the current training instance.\n",
    "\n",
    "yj' = output of the jth output neuron for the current training instance.\n",
    "\n",
    "yj = target output of the jth output neuron for the current training instance.\n",
    "\n",
    "n = learning rate.\n",
    "\n",
    "\n",
    "The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns. However, if the training instances are linearly separable then this algorithm would converge to a solution. This is called Perceptron convergence theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6d4cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]\n",
    "y = (iris.target == 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185c1499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d45e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2d74021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e81098",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron and Backpropagation\n",
    "\n",
    "An MLP is composed of one input layer, one or more layers of TLUs, called hidden layers, and one final layer of TLUs called the output layer. The layers close to the input layer are usually called the lower layers, and the ones close to the outputs are usually called the upper layers. Every layer except the output layer includes a bias neuron and is fully connected to the next layer. \n",
    "\n",
    "The architecture in which the signal flows only in one direction is an example of a feedforward neural network (FNN).\n",
    "\n",
    "When an ANN contains a deep stack of hidden layers, it is called a deep neural network.\n",
    "\n",
    "\n",
    "In 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a groundbreaking paper introducing the backpropagation training algorithm. It trains a neural network by adjusting its parameters using chain rule and gradients.\n",
    "\n",
    "At first we initialize random values to weights and bias terms. Each mini-batch is passed to hte network's input layer and computes the output of all the neurons in the layer and eventually the output. It is called forward pass. Next, the algorithm measures the networks output error. Then we need to compute the new weights and bias using chain rule. \n",
    "\n",
    "\n",
    "In order for this algorithm to work properly, the step function is replaced by the logistic function. This is due to the step function contains only the flat segments, so there is no gradient to work with, while the logistic function has a well-defined nonzero derivative every where, allowing Gradient Descent to make some progress at every step.\n",
    "\n",
    "The hyperbolic tangent function tanhz is just like logistic function is S-shaped, continuous, and differentiable, but its output values ranges from -1 to 1, which tends to make each layer's output more or less centered around 0 at the beginning of training. This often helps us convergence.\n",
    "\n",
    "The Rectified Linear Unit function: It is continuous but not differentiable at z=0 and its derivative is 0 for z<0. It is fast to compute.\n",
    "\n",
    "\n",
    "#### Regression MLPs\n",
    "We don't need to use any activation function for the output neurons, so they are free to ouput any range of values. \n",
    "\n",
    "The loss function is the mean squared error, but if there are lot of outliers in the training set, we cna use the mean absolute error. Alternatively, we can use the Huber loss, which is combination of both.\n",
    "\n",
    "The Huber loss is quadratic when the error is smaller than a threshold, but linear when the error is larger than del. This makes it less sensitive to outliers than the mean squared error, and it is often  more precise and converges faster than the mean absolute error. \n",
    "\n",
    "#### Classification MLPs\n",
    "For a binary classification problem, we need a single output neuron using the logistic activation function: the output will be a number between 0 and 1. When wee need to do multi-class classification then we should use teh softmax activation function for the whole output layer. \n",
    "\n",
    "The loss function used is cross-entropy(log loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36827d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
